{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rEn_due0NCI",
        "outputId": "ce5a9142-3952-4c56-f03a-6c7c0b756b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASM1-Development-LLM-model'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 72 (delta 24), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (72/72), 3.16 MiB | 8.25 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thviet79/ASM1-Development-LLM-model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "dtype=torch.float32"
      ],
      "metadata": {
        "id": "vsteCCB-1dbZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Helper function to reshape frequency tensor to have the same shape as the target tensor 'x'\n",
        "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
        "\n",
        "    Args:\n",
        "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
        "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reshaped frequency tensor.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
        "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
        "    \"\"\"\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(shape)\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    head_dim: int,\n",
        "    max_seq_len: int,\n",
        "    theta: float = 10000.0,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
        "\n",
        "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
        "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
        "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
        "    returned as real tensors.\n",
        "\n",
        "    Args:\n",
        "        query (torch.Tensor): Query tensor to apply rotary embeddings.\n",
        "                              Shape: (batch_size, seqlen, n_local_heads, self.head_dim)\n",
        "        key (torch.Tensor): Key tensor to apply rotary embeddings.\n",
        "                              Shape: (batch_size, seqlen, n_local_kv_heads, self.head_dim)\n",
        "        head_dim (int): Dimension of each attention head.\n",
        "        max_seq_len (int): Maximum sequence length supported by model.\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    _, seqlen, _, _ = query.shape\n",
        "    device = query.device\n",
        "\n",
        "    # reshape xq and xk to match the complex representation\n",
        "    query_real, query_imag = query.float().reshape(query.shape[:-1] + (-1, 2)).unbind(-1)\n",
        "    key_real, key_imag = key.float().reshape(key.shape[:-1] + (-1, 2)).unbind(-1)\n",
        "\n",
        "    # First, compute the trigonometric values in the second and fourth columns in\n",
        "    # slide 22 (linked above).\n",
        "    freqs = torch.pow(theta, -torch.arange(0, head_dim, 2, device=device)[:(head_dim//2)].float() / head_dim)\n",
        "    pos = torch.arange(seqlen, device=device).float()[:max_seq_len]\n",
        "\n",
        "    freqs = torch.outer(freqs, pos).transpose(-2, -1).float()  # (head_dim // 2, max_seq_len)\n",
        "    freqs = reshape_for_broadcast(freqs, query_real)\n",
        "\n",
        "    # shape: (batch_size, seqlen, n_local_heads, head_dim // 2)\n",
        "    query_rotated_real = freqs.cos() * query_real - freqs.sin() * query_imag\n",
        "    query_rotated_imag = freqs.sin() * query_real + freqs.cos() * query_imag\n",
        "    key_rotated_real = freqs.cos() * key_real - freqs.sin() * key_imag\n",
        "    key_rotated_imag = freqs.sin() * key_real + freqs.cos() * key_imag\n",
        "\n",
        "    # Then, combine these trigonometric values with the tensors query_real, query_imag,\n",
        "    # key_real, and key_imag.\n",
        "    query_stack = torch.stack((query_rotated_real, query_rotated_imag), dim=-1)\n",
        "    key_stack = torch.stack((key_rotated_real, key_rotated_imag), dim=-1)\n",
        "\n",
        "    query_out = query_stack.reshape(query.shape)\n",
        "    key_out = key_stack.reshape(key.shape)\n",
        "\n",
        "    # Return the rotary position embeddings for the query and key tensors\n",
        "    return query_out, key_out"
      ],
      "metadata": {
        "id": "WDEgoKHIoMWa"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "seed = 0\n",
        "\n",
        "def construct_query() -> torch.Tensor:\n",
        "    '''\n",
        "    Shape: (batch_size, seqlen, n_local_heads, self.head_dim)\n",
        "    '''\n",
        "    return 2 * torch.ones([1, 2, 2, 4])\n",
        "\n",
        "def construct_key() -> torch.Tensor:\n",
        "    '''\n",
        "    Shape: (batch_size, seqlen, n_local_kv_heads, self.head_dim)\n",
        "    '''\n",
        "    return 3 * torch.ones([1, 2, 2, 4])\n",
        "\n",
        "def test_apply_rotary_emb() -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    model = torch.nn.Linear(3, 2, bias=False)\n",
        "\n",
        "    test_query = construct_query()\n",
        "    test_key = construct_key()\n",
        "    rotary_embeddings = apply_rotary_emb(test_query, test_key, 4, 20)\n",
        "    rotary_query_embedding, rotary_key_embedding = rotary_embeddings\n",
        "    return rotary_query_embedding, rotary_key_embedding\n",
        "\n",
        "actual_query_rope_embedding, actual_key_rope_embedding = test_apply_rotary_emb()\n",
        "ref_query_rope_embedding, ref_key_rope_embedding = torch.load(\"/content/ASM1-Development-LLM-model/rotary_embedding_actual.data\")\n",
        "\n",
        "assert torch.allclose(ref_query_rope_embedding, actual_query_rope_embedding)\n",
        "assert torch.allclose(ref_key_rope_embedding, actual_key_rope_embedding)\n",
        "print(\"Rotary embedding test passed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUfuOb991myg",
        "outputId": "3ee876f6-8922-412c-cff9-cebe3e39ab83"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rotary embedding test passed!\n"
          ]
        }
      ]
    }
  ]
}